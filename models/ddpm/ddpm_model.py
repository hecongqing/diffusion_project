#!/usr/bin/env python3
"""
DDPM (Denoising Diffusion Probabilistic Models) å®ç°
=================================================

è¿™ä¸ªæ–‡ä»¶å®ç°äº†å®Œæ•´çš„DDPMæ¨¡å‹ï¼ŒåŒ…æ‹¬ï¼š
1. å™ªå£°è°ƒåº¦å™¨ (Noise Scheduler)
2. UNetå»å™ªç½‘ç»œ (Denoising Network)  
3. è®­ç»ƒå’Œæ¨ç†é€»è¾‘ (Training & Inference)

å‚è€ƒè®ºæ–‡:
- Denoising Diffusion Probabilistic Models (Ho et al., 2020)
- https://arxiv.org/abs/2006.11239

ä½œè€…: Diffusionæ•™ç¨‹å›¢é˜Ÿ
æ—¥æœŸ: 2024å¹´
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Optional, Tuple, List
import math

class NoiseScheduler:
    """
    DDPMå™ªå£°è°ƒåº¦å™¨
    
    è´Ÿè´£ç®¡ç†æ‰©æ•£è¿‡ç¨‹ä¸­çš„å™ªå£°æ·»åŠ å’Œå‚æ•°è®¡ç®—ã€‚
    å®ç°äº†çº¿æ€§å™ªå£°è°ƒåº¦ç­–ç•¥ã€‚
    """
    
    def __init__(
        self, 
        num_timesteps: int = 1000,
        beta_start: float = 0.0001,
        beta_end: float = 0.02,
        schedule: str = "linear"
    ):
        """
        åˆå§‹åŒ–å™ªå£°è°ƒåº¦å™¨
        
        å‚æ•°:
            num_timesteps: æ‰©æ•£æ€»æ­¥æ•° T
            beta_start: å™ªå£°è°ƒåº¦èµ·å§‹å€¼
            beta_end: å™ªå£°è°ƒåº¦ç»“æŸå€¼
            schedule: è°ƒåº¦ç­–ç•¥ ("linear" æˆ– "cosine")
        """
        self.num_timesteps = num_timesteps
        self.beta_start = beta_start
        self.beta_end = beta_end
        
        # 1. è®¡ç®—å™ªå£°è°ƒåº¦ Î²_t
        if schedule == "linear":
            # çº¿æ€§è°ƒåº¦ï¼šÎ²_t ä» beta_start çº¿æ€§å¢é•¿åˆ° beta_end
            self.betas = torch.linspace(beta_start, beta_end, num_timesteps)
        elif schedule == "cosine":
            # ä½™å¼¦è°ƒåº¦ï¼šæ›´å¹³æ»‘çš„å™ªå£°æ·»åŠ ç­–ç•¥
            self.betas = self._cosine_beta_schedule()
        else:
            raise ValueError(f"æœªçŸ¥çš„è°ƒåº¦ç­–ç•¥: {schedule}")
        
        # 2. è®¡ç®—ç›¸å…³å‚æ•°
        self.alphas = 1.0 - self.betas  # Î±_t = 1 - Î²_t
        
        # ç´¯ç§¯ä¹˜ç§¯ï¼šá¾±_t = âˆ(Î±_i) for i=1 to t
        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)
        
        # å‰ä¸€æ­¥çš„ç´¯ç§¯ä¹˜ç§¯ï¼šá¾±_{t-1}
        self.alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1, 0), value=1.0)
        
        # 3. é¢„è®¡ç®—é‡‡æ ·æ‰€éœ€çš„ç³»æ•°
        # ç”¨äºå‰å‘è¿‡ç¨‹ï¼šq(x_t|x_0)
        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)  # âˆšá¾±_t
        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.alphas_cumprod)  # âˆš(1-á¾±_t)
        
        # ç”¨äºåå‘è¿‡ç¨‹ï¼šp(x_{t-1}|x_t, x_0)
        self.sqrt_recip_alphas = torch.sqrt(1.0 / self.alphas)  # 1/âˆšÎ±_t
        
        # åéªŒæ–¹å·®ï¼šÎ²Ìƒ_t = Î²_t * (1-á¾±_{t-1})/(1-á¾±_t)
        self.posterior_variance = (
            self.betas * (1.0 - self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)
        )
        
        # åéªŒå‡å€¼ç³»æ•°
        self.posterior_mean_coef1 = (
            self.betas * torch.sqrt(self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)
        )
        self.posterior_mean_coef2 = (
            (1.0 - self.alphas_cumprod_prev) * torch.sqrt(self.alphas) / (1.0 - self.alphas_cumprod)
        )
    
    def _cosine_beta_schedule(self) -> torch.Tensor:
        """
        ä½™å¼¦å™ªå£°è°ƒåº¦ç­–ç•¥
        
        ç›¸æ¯”çº¿æ€§è°ƒåº¦ï¼Œä½™å¼¦è°ƒåº¦åœ¨è®­ç»ƒæ—©æœŸæ·»åŠ æ›´å°‘å™ªå£°ï¼Œ
        åœ¨åæœŸæ·»åŠ æ›´å¤šå™ªå£°ï¼Œé€šå¸¸èƒ½è·å¾—æ›´å¥½çš„ç”Ÿæˆè´¨é‡ã€‚
        """
        s = 0.008  # å°çš„åç§»é‡
        steps = self.num_timesteps + 1
        x = torch.linspace(0, self.num_timesteps, steps)
        
        # ä½™å¼¦å‡½æ•°
        alphas_cumprod = torch.cos(((x / self.num_timesteps) + s) / (1 + s) * math.pi * 0.5) ** 2
        alphas_cumprod = alphas_cumprod / alphas_cumprod[0]
        
        # è®¡ç®— Î²_t
        betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])
        return torch.clip(betas, 0, 0.999)
    
    def add_noise(
        self, 
        original_samples: torch.Tensor, 
        noise: torch.Tensor, 
        timesteps: torch.Tensor
    ) -> torch.Tensor:
        """
        å‰å‘æ‰©æ•£è¿‡ç¨‹ï¼šå‘åŸå§‹æ ·æœ¬æ·»åŠ å™ªå£°
        
        å®ç°å…¬å¼ï¼šx_t = âˆšá¾±_t * x_0 + âˆš(1-á¾±_t) * Îµ
        
        å‚æ•°:
            original_samples: åŸå§‹æ ·æœ¬ x_0 [batch_size, channels, height, width]
            noise: é«˜æ–¯å™ªå£° Îµ [batch_size, channels, height, width]  
            timesteps: æ—¶é—´æ­¥ t [batch_size]
            
        è¿”å›:
            noisy_samples: æ·»åŠ å™ªå£°åçš„æ ·æœ¬ x_t
        """
        # è·å–å¯¹åº”æ—¶é—´æ­¥çš„ç³»æ•°
        sqrt_alpha_cumprod = self.sqrt_alphas_cumprod[timesteps]
        sqrt_one_minus_alpha_cumprod = self.sqrt_one_minus_alphas_cumprod[timesteps]
        
        # è°ƒæ•´å½¢çŠ¶ä»¥ä¾¿å¹¿æ’­ [batch_size] -> [batch_size, 1, 1, 1]
        sqrt_alpha_cumprod = sqrt_alpha_cumprod.view(-1, 1, 1, 1)
        sqrt_one_minus_alpha_cumprod = sqrt_one_minus_alpha_cumprod.view(-1, 1, 1, 1)
        
        # åº”ç”¨é‡å‚æ•°åŒ–æŠ€å·§ï¼šx_t = âˆšá¾±_t * x_0 + âˆš(1-á¾±_t) * Îµ
        noisy_samples = (
            sqrt_alpha_cumprod * original_samples + 
            sqrt_one_minus_alpha_cumprod * noise
        )
        
        return noisy_samples
    
    def step(
        self, 
        model_output: torch.Tensor, 
        timestep: int, 
        sample: torch.Tensor,
        generator: Optional[torch.Generator] = None
    ) -> torch.Tensor:
        """
        åå‘å»å™ªæ­¥éª¤ï¼šä» x_t é¢„æµ‹ x_{t-1}
        
        å®ç°DDPMçš„é‡‡æ ·å…¬å¼ï¼š
        x_{t-1} = 1/âˆšÎ±_t * (x_t - Î²_t/âˆš(1-á¾±_t) * Îµ_Î¸(x_t,t)) + Ïƒ_t * z
        
        å‚æ•°:
            model_output: æ¨¡å‹é¢„æµ‹çš„å™ªå£° Îµ_Î¸(x_t,t)
            timestep: å½“å‰æ—¶é—´æ­¥ t
            sample: å½“å‰æ ·æœ¬ x_t
            generator: éšæœºæ•°ç”Ÿæˆå™¨
            
        è¿”å›:
            prev_sample: å‰ä¸€æ­¥æ ·æœ¬ x_{t-1}
        """
        # è·å–å½“å‰æ—¶é—´æ­¥çš„å‚æ•°
        alpha_t = self.alphas[timestep]
        alpha_cumprod_t = self.alphas_cumprod[timestep]
        beta_t = self.betas[timestep]
        
        # 1. è®¡ç®—é¢„æµ‹çš„åŸå§‹æ ·æœ¬ x_0
        # x_0 = (x_t - âˆš(1-á¾±_t) * Îµ_Î¸) / âˆšá¾±_t
        pred_original_sample = (
            sample - torch.sqrt(1 - alpha_cumprod_t) * model_output
        ) / torch.sqrt(alpha_cumprod_t)
        
        # 2. è®¡ç®—å‰ä¸€æ­¥æ ·æœ¬çš„å‡å€¼
        # Î¼ = 1/âˆšÎ±_t * (x_t - Î²_t/âˆš(1-á¾±_t) * Îµ_Î¸)
        pred_prev_mean = (
            1.0 / torch.sqrt(alpha_t) * 
            (sample - beta_t / torch.sqrt(1 - alpha_cumprod_t) * model_output)
        )
        
        # 3. æ·»åŠ å™ªå£°ï¼ˆé™¤äº†æœ€åä¸€æ­¥ï¼‰
        if timestep > 0:
            variance = self.posterior_variance[timestep]
            # ç”Ÿæˆæ ‡å‡†é«˜æ–¯å™ªå£°
            noise = torch.randn_like(sample, generator=generator)
            # æ·»åŠ æ–¹å·®ç¼©æ”¾çš„å™ªå£°
            prev_sample = pred_prev_mean + torch.sqrt(variance) * noise
        else:
            # æœ€åä¸€æ­¥ä¸æ·»åŠ å™ªå£°
            prev_sample = pred_prev_mean
        
        return prev_sample


class TimeEmbedding(nn.Module):
    """
    æ—¶é—´æ­¥åµŒå…¥æ¨¡å—
    
    å°†æ—¶é—´æ­¥ t ç¼–ç ä¸ºé«˜ç»´å‘é‡ï¼Œç”¨äºæŒ‡å¯¼å»å™ªç½‘ç»œã€‚
    ä½¿ç”¨æ­£å¼¦ä½ç½®ç¼–ç çš„å˜ä½“ã€‚
    """
    
    def __init__(self, dim: int):
        """
        å‚æ•°:
            dim: åµŒå…¥ç»´åº¦
        """
        super().__init__()
        self.dim = dim
        
        # æŠ•å½±å±‚
        self.proj = nn.Sequential(
            nn.Linear(dim, dim * 4),
            nn.SiLU(),  # Swishæ¿€æ´»å‡½æ•°
            nn.Linear(dim * 4, dim * 4),
            nn.SiLU(),
            nn.Linear(dim * 4, dim)
        )
    
    def forward(self, timesteps: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        å‚æ•°:
            timesteps: æ—¶é—´æ­¥ [batch_size]
            
        è¿”å›:
            åµŒå…¥å‘é‡ [batch_size, dim]
        """
        # 1. æ­£å¼¦ä½ç½®ç¼–ç 
        half_dim = self.dim // 2
        embeddings = math.log(10000) / (half_dim - 1)
        embeddings = torch.exp(torch.arange(half_dim, device=timesteps.device) * -embeddings)
        embeddings = timesteps[:, None] * embeddings[None, :]
        
        # 2. æ­£å¼¦å’Œä½™å¼¦
        embeddings = torch.cat([torch.sin(embeddings), torch.cos(embeddings)], dim=-1)
        
        # 3. é€šè¿‡æŠ•å½±ç½‘ç»œ
        embeddings = self.proj(embeddings)
        
        return embeddings


class ResidualBlock(nn.Module):
    """
    æ®‹å·®å—
    
    UNetçš„åŸºæœ¬æ„å»ºæ¨¡å—ï¼ŒåŒ…å«ï¼š
    - ç»„å½’ä¸€åŒ–
    - å·ç§¯å±‚  
    - æ—¶é—´åµŒå…¥æ³¨å…¥
    - æ®‹å·®è¿æ¥
    """
    
    def __init__(
        self, 
        in_channels: int, 
        out_channels: int, 
        time_emb_dim: int,
        dropout: float = 0.1
    ):
        super().__init__()
        
        # ç¬¬ä¸€ä¸ªå·ç§¯å—
        self.norm1 = nn.GroupNorm(8, in_channels)
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)
        
        # æ—¶é—´åµŒå…¥æŠ•å½±
        self.time_proj = nn.Linear(time_emb_dim, out_channels)
        
        # ç¬¬äºŒä¸ªå·ç§¯å—
        self.norm2 = nn.GroupNorm(8, out_channels)
        self.dropout = nn.Dropout(dropout)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)
        
        # æ®‹å·®è¿æ¥
        if in_channels != out_channels:
            self.residual_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)
        else:
            self.residual_conv = nn.Identity()
    
    def forward(self, x: torch.Tensor, time_emb: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        å‚æ•°:
            x: è¾“å…¥ç‰¹å¾ [batch_size, in_channels, height, width]
            time_emb: æ—¶é—´åµŒå…¥ [batch_size, time_emb_dim]
            
        è¿”å›:
            è¾“å‡ºç‰¹å¾ [batch_size, out_channels, height, width]
        """
        # ä¿å­˜æ®‹å·®
        residual = self.residual_conv(x)
        
        # ç¬¬ä¸€ä¸ªå·ç§¯å—
        h = self.norm1(x)
        h = F.silu(h)  # Swishæ¿€æ´»
        h = self.conv1(h)
        
        # æ³¨å…¥æ—¶é—´ä¿¡æ¯
        time_emb_proj = self.time_proj(time_emb)  # [batch_size, out_channels]
        h = h + time_emb_proj[:, :, None, None]  # å¹¿æ’­åˆ°ç©ºé—´ç»´åº¦
        
        # ç¬¬äºŒä¸ªå·ç§¯å—
        h = self.norm2(h)
        h = F.silu(h)
        h = self.dropout(h)
        h = self.conv2(h)
        
        # æ®‹å·®è¿æ¥
        return h + residual


class AttentionBlock(nn.Module):
    """
    è‡ªæ³¨æ„åŠ›å—
    
    åœ¨UNetçš„ç‰¹å®šå±‚ä½¿ç”¨ï¼Œå¸®åŠ©æ¨¡å‹å…³æ³¨å›¾åƒçš„ä¸åŒåŒºåŸŸã€‚
    """
    
    def __init__(self, channels: int):
        super().__init__()
        self.channels = channels
        
        self.norm = nn.GroupNorm(8, channels)
        self.qkv = nn.Conv2d(channels, channels * 3, kernel_size=1)
        self.proj = nn.Conv2d(channels, channels, kernel_size=1)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        å‚æ•°:
            x: è¾“å…¥ç‰¹å¾ [batch_size, channels, height, width]
            
        è¿”å›:
            è¾“å‡ºç‰¹å¾ [batch_size, channels, height, width]
        """
        residual = x
        batch_size, channels, height, width = x.shape
        
        # å½’ä¸€åŒ–
        h = self.norm(x)
        
        # è®¡ç®— Q, K, V
        qkv = self.qkv(h)  # [batch_size, channels*3, height, width]
        q, k, v = qkv.chunk(3, dim=1)  # æ¯ä¸ªéƒ½æ˜¯ [batch_size, channels, height, width]
        
        # é‡å¡‘ä¸ºåºåˆ—å½¢å¼
        q = q.view(batch_size, channels, height * width).transpose(1, 2)  # [batch_size, hw, channels]
        k = k.view(batch_size, channels, height * width).transpose(1, 2)  # [batch_size, hw, channels]  
        v = v.view(batch_size, channels, height * width).transpose(1, 2)  # [batch_size, hw, channels]
        
        # è®¡ç®—æ³¨æ„åŠ›
        scale = 1.0 / math.sqrt(channels)
        attn = torch.bmm(q, k.transpose(1, 2)) * scale  # [batch_size, hw, hw]
        attn = F.softmax(attn, dim=-1)
        
        # åº”ç”¨æ³¨æ„åŠ›
        h = torch.bmm(attn, v)  # [batch_size, hw, channels]
        
        # é‡å¡‘å›ç©ºé—´å½¢å¼
        h = h.transpose(1, 2).view(batch_size, channels, height, width)
        
        # æŠ•å½±
        h = self.proj(h)
        
        return h + residual


class UNet(nn.Module):
    """
    UNetå»å™ªç½‘ç»œ
    
    æ‰©æ•£æ¨¡å‹çš„æ ¸å¿ƒç½‘ç»œï¼Œè´Ÿè´£é¢„æµ‹å™ªå£°ã€‚
    é‡‡ç”¨ç¼–ç å™¨-è§£ç å™¨ç»“æ„ï¼Œå…·æœ‰è·³è·ƒè¿æ¥ã€‚
    """
    
    def __init__(
        self,
        in_channels: int = 3,
        out_channels: int = 3,
        model_channels: int = 128,
        num_res_blocks: int = 2,
        attention_resolutions: Tuple[int, ...] = (16, 8),
        channel_mult: Tuple[int, ...] = (1, 2, 4, 8),
        dropout: float = 0.1
    ):
        """
        åˆå§‹åŒ–UNet
        
        å‚æ•°:
            in_channels: è¾“å…¥é€šé“æ•°
            out_channels: è¾“å‡ºé€šé“æ•°  
            model_channels: åŸºç¡€é€šé“æ•°
            num_res_blocks: æ¯ä¸ªåˆ†è¾¨ç‡çº§åˆ«çš„æ®‹å·®å—æ•°é‡
            attention_resolutions: ä½¿ç”¨æ³¨æ„åŠ›çš„åˆ†è¾¨ç‡çº§åˆ«
            channel_mult: é€šé“æ•°å€å¢å› å­
            dropout: Dropoutæ¦‚ç‡
        """
        super().__init__()
        
        self.in_channels = in_channels
        self.model_channels = model_channels
        self.num_res_blocks = num_res_blocks
        self.attention_resolutions = attention_resolutions
        self.channel_mult = channel_mult
        
        # æ—¶é—´åµŒå…¥
        time_embed_dim = model_channels * 4
        self.time_embed = TimeEmbedding(time_embed_dim)
        
        # è¾“å…¥æŠ•å½±
        self.input_conv = nn.Conv2d(in_channels, model_channels, kernel_size=3, padding=1)
        
        # ç¼–ç å™¨
        self.down_blocks = nn.ModuleList()
        self.down_samples = nn.ModuleList()
        
        ch = model_channels
        input_ch = model_channels
        for level, mult in enumerate(channel_mult):
            out_ch = model_channels * mult
            
            # æ®‹å·®å—
            for _ in range(num_res_blocks):
                block = ResidualBlock(input_ch, out_ch, time_embed_dim, dropout)
                self.down_blocks.append(block)
                input_ch = out_ch
                
                # æ·»åŠ æ³¨æ„åŠ›ï¼ˆå¦‚æœéœ€è¦ï¼‰
                if 32 // (2 ** level) in attention_resolutions:
                    attn = AttentionBlock(out_ch)
                    self.down_blocks.append(attn)
            
            # ä¸‹é‡‡æ ·ï¼ˆé™¤äº†æœ€åä¸€å±‚ï¼‰
            if level < len(channel_mult) - 1:
                downsample = nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=2, padding=1)
                self.down_samples.append(downsample)
            else:
                self.down_samples.append(nn.Identity())
        
        # ä¸­é—´å—
        mid_ch = model_channels * channel_mult[-1]
        self.mid_block1 = ResidualBlock(mid_ch, mid_ch, time_embed_dim, dropout)
        self.mid_attn = AttentionBlock(mid_ch)
        self.mid_block2 = ResidualBlock(mid_ch, mid_ch, time_embed_dim, dropout)
        
        # è§£ç å™¨
        self.up_blocks = nn.ModuleList()
        self.up_samples = nn.ModuleList()
        
        for level, mult in enumerate(reversed(channel_mult)):
            out_ch = model_channels * mult
            
            # æ®‹å·®å—
            for i in range(num_res_blocks + 1):
                # ç¬¬ä¸€ä¸ªå—éœ€è¦å¤„ç†è·³è·ƒè¿æ¥
                in_ch = input_ch + (out_ch if i == 0 else 0)
                block = ResidualBlock(in_ch, out_ch, time_embed_dim, dropout)
                self.up_blocks.append(block)
                input_ch = out_ch
                
                # æ·»åŠ æ³¨æ„åŠ›ï¼ˆå¦‚æœéœ€è¦ï¼‰
                resolution = 32 // (2 ** (len(channel_mult) - 1 - level))
                if resolution in attention_resolutions:
                    attn = AttentionBlock(out_ch)
                    self.up_blocks.append(attn)
            
            # ä¸Šé‡‡æ ·ï¼ˆé™¤äº†æœ€åä¸€å±‚ï¼‰
            if level < len(channel_mult) - 1:
                upsample = nn.ConvTranspose2d(out_ch, out_ch, kernel_size=4, stride=2, padding=1)
                self.up_samples.append(upsample)
            else:
                self.up_samples.append(nn.Identity())
        
        # è¾“å‡ºæŠ•å½±
        self.output_norm = nn.GroupNorm(8, model_channels)
        self.output_conv = nn.Conv2d(model_channels, out_channels, kernel_size=3, padding=1)
    
    def forward(self, x: torch.Tensor, timesteps: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        å‚æ•°:
            x: å™ªå£°å›¾åƒ [batch_size, channels, height, width]
            timesteps: æ—¶é—´æ­¥ [batch_size]
            
        è¿”å›:
            é¢„æµ‹çš„å™ªå£° [batch_size, channels, height, width]
        """
        # æ—¶é—´åµŒå…¥
        time_emb = self.time_embed(timesteps)
        
        # è¾“å…¥æŠ•å½±
        h = self.input_conv(x)
        
        # ç¼–ç å™¨è·¯å¾„ï¼ˆä¿å­˜è·³è·ƒè¿æ¥ï¼‰
        skip_connections = [h]
        
        block_idx = 0
        for level in range(len(self.channel_mult)):
            # æ®‹å·®å—
            for _ in range(self.num_res_blocks):
                h = self.down_blocks[block_idx](h, time_emb)
                block_idx += 1
                
                # æ³¨æ„åŠ›å—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if isinstance(self.down_blocks[block_idx], AttentionBlock):
                    h = self.down_blocks[block_idx](h)
                    block_idx += 1
                
                skip_connections.append(h)
            
            # ä¸‹é‡‡æ ·
            h = self.down_samples[level](h)
            if level < len(self.channel_mult) - 1:
                skip_connections.append(h)
        
        # ä¸­é—´å—
        h = self.mid_block1(h, time_emb)
        h = self.mid_attn(h)
        h = self.mid_block2(h, time_emb)
        
        # è§£ç å™¨è·¯å¾„ï¼ˆä½¿ç”¨è·³è·ƒè¿æ¥ï¼‰
        block_idx = 0
        for level in range(len(self.channel_mult)):
            # æ®‹å·®å—
            for i in range(self.num_res_blocks + 1):
                # ç¬¬ä¸€ä¸ªå—è¿æ¥è·³è·ƒè¿æ¥
                if i == 0:
                    skip = skip_connections.pop()
                    h = torch.cat([h, skip], dim=1)
                
                h = self.up_blocks[block_idx](h, time_emb)
                block_idx += 1
                
                # æ³¨æ„åŠ›å—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if isinstance(self.up_blocks[block_idx], AttentionBlock):
                    h = self.up_blocks[block_idx](h)
                    block_idx += 1
            
            # ä¸Šé‡‡æ ·
            if level < len(self.channel_mult) - 1:
                h = self.up_samples[level](h)
        
        # è¾“å‡ºæŠ•å½±
        h = self.output_norm(h)
        h = F.silu(h)
        h = self.output_conv(h)
        
        return h


class DDPM(nn.Module):
    """
    å®Œæ•´çš„DDPMæ¨¡å‹
    
    é›†æˆäº†å™ªå£°è°ƒåº¦å™¨å’ŒUNetç½‘ç»œï¼Œæä¾›è®­ç»ƒå’Œæ¨ç†æ¥å£ã€‚
    """
    
    def __init__(
        self,
        unet: UNet,
        noise_scheduler: NoiseScheduler
    ):
        """
        å‚æ•°:
            unet: UNetå»å™ªç½‘ç»œ
            noise_scheduler: å™ªå£°è°ƒåº¦å™¨
        """
        super().__init__()
        self.unet = unet
        self.noise_scheduler = noise_scheduler
    
    def forward(self, x: torch.Tensor, timesteps: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­ï¼ˆè®­ç»ƒæ—¶ä½¿ç”¨ï¼‰
        
        å‚æ•°:
            x: å™ªå£°å›¾åƒ
            timesteps: æ—¶é—´æ­¥
            
        è¿”å›:
            é¢„æµ‹çš„å™ªå£°
        """
        return self.unet(x, timesteps)
    
    def training_step(self, batch: torch.Tensor) -> torch.Tensor:
        """
        è®­ç»ƒæ­¥éª¤
        
        å‚æ•°:
            batch: å¹²å‡€å›¾åƒæ‰¹æ¬¡ [batch_size, channels, height, width]
            
        è¿”å›:
            æŸå¤±å€¼
        """
        batch_size = batch.shape[0]
        device = batch.device
        
        # 1. éšæœºé‡‡æ ·æ—¶é—´æ­¥
        timesteps = torch.randint(
            0, self.noise_scheduler.num_timesteps, 
            (batch_size,), device=device
        )
        
        # 2. ç”Ÿæˆéšæœºå™ªå£°
        noise = torch.randn_like(batch)
        
        # 3. å‰å‘æ‰©æ•£ï¼šæ·»åŠ å™ªå£°
        noisy_images = self.noise_scheduler.add_noise(batch, noise, timesteps)
        
        # 4. é¢„æµ‹å™ªå£°
        noise_pred = self.unet(noisy_images, timesteps)
        
        # 5. è®¡ç®—MSEæŸå¤±
        loss = F.mse_loss(noise_pred, noise)
        
        return loss
    
    @torch.no_grad()
    def sample(
        self, 
        batch_size: int = 1,
        image_size: int = 32,
        channels: int = 3,
        device: str = "cpu",
        generator: Optional[torch.Generator] = None
    ) -> torch.Tensor:
        """
        é‡‡æ ·ç”Ÿæˆå›¾åƒ
        
        å‚æ•°:
            batch_size: æ‰¹æ¬¡å¤§å°
            image_size: å›¾åƒå°ºå¯¸
            channels: é€šé“æ•°
            device: è®¾å¤‡
            generator: éšæœºæ•°ç”Ÿæˆå™¨
            
        è¿”å›:
            ç”Ÿæˆçš„å›¾åƒ [batch_size, channels, image_size, image_size]
        """
        # ä»çº¯å™ªå£°å¼€å§‹
        shape = (batch_size, channels, image_size, image_size)
        image = torch.randn(shape, device=device, generator=generator)
        
        # é€æ­¥å»å™ª
        for t in range(self.noise_scheduler.num_timesteps - 1, -1, -1):
            # é¢„æµ‹å™ªå£°
            timesteps = torch.full((batch_size,), t, device=device, dtype=torch.long)
            noise_pred = self.unet(image, timesteps)
            
            # å»å™ªä¸€æ­¥
            image = self.noise_scheduler.step(noise_pred, t, image, generator)
        
        return image


# å·¥å‚å‡½æ•°
def create_ddpm_model(
    image_size: int = 32,
    channels: int = 3,
    num_timesteps: int = 1000
) -> DDPM:
    """
    åˆ›å»ºDDPMæ¨¡å‹çš„å·¥å‚å‡½æ•°
    
    å‚æ•°:
        image_size: å›¾åƒå°ºå¯¸
        channels: é€šé“æ•°
        num_timesteps: æ‰©æ•£æ­¥æ•°
        
    è¿”å›:
        é…ç½®å¥½çš„DDPMæ¨¡å‹
    """
    # åˆ›å»ºå™ªå£°è°ƒåº¦å™¨
    noise_scheduler = NoiseScheduler(num_timesteps=num_timesteps)
    
    # åˆ›å»ºUNetç½‘ç»œ
    unet = UNet(
        in_channels=channels,
        out_channels=channels,
        model_channels=128,
        num_res_blocks=2,
        attention_resolutions=(16, 8),
        channel_mult=(1, 2, 4, 8) if image_size >= 64 else (1, 2, 4),
        dropout=0.1
    )
    
    # åˆ›å»ºå®Œæ•´æ¨¡å‹
    model = DDPM(unet, noise_scheduler)
    
    return model


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("ğŸ§ª æµ‹è¯•DDPMæ¨¡å‹...")
    
    # åˆ›å»ºæ¨¡å‹
    model = create_ddpm_model(image_size=32, channels=3, num_timesteps=100)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size = 4
    test_images = torch.randn(batch_size, 3, 32, 32)
    
    # æµ‹è¯•è®­ç»ƒæ­¥éª¤
    loss = model.training_step(test_images)
    print(f"âœ… è®­ç»ƒæŸå¤±: {loss.item():.4f}")
    
    # æµ‹è¯•é‡‡æ ·
    with torch.no_grad():
        samples = model.sample(batch_size=2, image_size=32, channels=3)
        print(f"âœ… é‡‡æ ·å½¢çŠ¶: {samples.shape}")
    
    print("ğŸ‰ DDPMæ¨¡å‹æµ‹è¯•å®Œæˆï¼")