#!/usr/bin/env python3
"""
UNetç½‘ç»œæ¶æ„å®ç° - Diffusionæ¨¡å‹çš„æ ¸å¿ƒç»„ä»¶
=======================================

UNetæ˜¯æ‰©æ•£æ¨¡å‹ä¸­æœ€é‡è¦çš„ç»„ä»¶ï¼Œè´Ÿè´£é¢„æµ‹å’Œå»é™¤å™ªå£°ã€‚
è¿™ä¸ªå®ç°åŒ…å«äº†ç°ä»£UNetçš„æ‰€æœ‰å…³é”®ç‰¹æ€§ï¼š
1. æ—¶é—´æ­¥åµŒå…¥ (Time Embedding)
2. æ®‹å·®è¿æ¥ (Residual Connections)
3. æ³¨æ„åŠ›æœºåˆ¶ (Attention Mechanisms)
4. è·³è·ƒè¿æ¥ (Skip Connections)

ä½œè€…: Diffusionæ•™ç¨‹å›¢é˜Ÿ
æ—¥æœŸ: 2024å¹´
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from typing import Optional, Tuple, List

class TimeEmbedding(nn.Module):
    """
    æ—¶é—´æ­¥åµŒå…¥å±‚
    
    å°†ç¦»æ•£çš„æ—¶é—´æ­¥tè½¬æ¢ä¸ºè¿ç»­çš„ç‰¹å¾è¡¨ç¤ºï¼Œ
    ä½¿æ¨¡å‹èƒ½å¤Ÿæ„ŸçŸ¥å½“å‰å¤„äºæ‰©æ•£è¿‡ç¨‹çš„å“ªä¸ªé˜¶æ®µã€‚
    """
    
    def __init__(self, dim: int):
        """
        åˆå§‹åŒ–æ—¶é—´åµŒå…¥å±‚
        
        å‚æ•°:
            dim: åµŒå…¥ç»´åº¦
        """
        super().__init__()
        self.dim = dim
        
        # ä½¿ç”¨æ­£å¼¦ä½ç½®ç¼–ç çš„æ€æƒ³
        # ä¸åŒé¢‘ç‡çš„æ­£å¼¦å’Œä½™å¼¦å‡½æ•°å¯ä»¥å”¯ä¸€ç¼–ç æ—¶é—´æ­¥
        half_dim = dim // 2
        self.register_buffer(
            'freqs',
            torch.exp(-math.log(10000) * torch.arange(half_dim) / half_dim)
        )
        
        # MLPå±‚å°†ä½ç½®ç¼–ç æ˜ å°„åˆ°æ›´é«˜ç»´ç©ºé—´
        self.mlp = nn.Sequential(
            nn.Linear(dim, dim * 4),
            nn.SiLU(),  # Swishæ¿€æ´»å‡½æ•°ï¼Œåœ¨æ‰©æ•£æ¨¡å‹ä¸­è¡¨ç°æ›´å¥½
            nn.Linear(dim * 4, dim),
        )
    
    def forward(self, timesteps: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        å‚æ•°:
            timesteps: æ—¶é—´æ­¥å¼ é‡ [batch_size]
            
        è¿”å›:
            time_emb: æ—¶é—´åµŒå…¥ [batch_size, dim]
        """
        # è®¡ç®—æ­£å¼¦ä½ç½®ç¼–ç 
        # timesteps: [batch_size] -> [batch_size, 1]
        timesteps = timesteps.unsqueeze(1).float()
        
        # å¹¿æ’­ç›¸ä¹˜: [batch_size, 1] * [half_dim] -> [batch_size, half_dim]
        args = timesteps * self.freqs
        
        # æ‹¼æ¥æ­£å¼¦å’Œä½™å¼¦ç¼–ç 
        embeddings = torch.cat([torch.cos(args), torch.sin(args)], dim=1)
        
        # é€šè¿‡MLPè¿›ä¸€æ­¥å¤„ç†
        return self.mlp(embeddings)


class ResidualBlock(nn.Module):
    """
    æ®‹å·®å— - UNetçš„åŸºæœ¬æ„å»ºå•å…ƒ
    
    åŒ…å«ï¼š
    1. GroupNormæ ‡å‡†åŒ–
    2. å·ç§¯å±‚
    3. æ—¶é—´åµŒå…¥æ³¨å…¥
    4. æ®‹å·®è¿æ¥
    """
    
    def __init__(
        self, 
        in_channels: int, 
        out_channels: int, 
        time_emb_dim: int,
        groups: int = 8
    ):
        """
        åˆå§‹åŒ–æ®‹å·®å—
        
        å‚æ•°:
            in_channels: è¾“å…¥é€šé“æ•°
            out_channels: è¾“å‡ºé€šé“æ•°  
            time_emb_dim: æ—¶é—´åµŒå…¥ç»´åº¦
            groups: GroupNormçš„ç»„æ•°
        """
        super().__init__()
        
        self.in_channels = in_channels
        self.out_channels = out_channels
        
        # ç¬¬ä¸€ä¸ªå·ç§¯è·¯å¾„
        self.norm1 = nn.GroupNorm(groups, in_channels)
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)
        
        # æ—¶é—´åµŒå…¥æŠ•å½±å±‚
        self.time_proj = nn.Linear(time_emb_dim, out_channels)
        
        # ç¬¬äºŒä¸ªå·ç§¯è·¯å¾„
        self.norm2 = nn.GroupNorm(groups, out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)
        
        # æ®‹å·®è¿æ¥çš„æŠ•å½±å±‚ï¼ˆå½“è¾“å…¥è¾“å‡ºé€šé“æ•°ä¸åŒæ—¶ï¼‰
        self.residual_proj = nn.Conv2d(in_channels, out_channels, 1) \
            if in_channels != out_channels else nn.Identity()
    
    def forward(self, x: torch.Tensor, time_emb: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        å‚æ•°:
            x: è¾“å…¥ç‰¹å¾å›¾ [batch_size, in_channels, height, width]
            time_emb: æ—¶é—´åµŒå…¥ [batch_size, time_emb_dim]
            
        è¿”å›:
            output: è¾“å‡ºç‰¹å¾å›¾ [batch_size, out_channels, height, width]
        """
        # ä¿å­˜æ®‹å·®è¿æ¥
        residual = self.residual_proj(x)
        
        # ç¬¬ä¸€ä¸ªå·ç§¯å—
        h = self.norm1(x)
        h = F.silu(h)  # Swishæ¿€æ´»
        h = self.conv1(h)
        
        # æ³¨å…¥æ—¶é—´ä¿¡æ¯
        # time_emb: [batch_size, time_emb_dim] -> [batch_size, out_channels]
        time_proj = self.time_proj(time_emb)
        # æ·»åŠ ç©ºé—´ç»´åº¦: [batch_size, out_channels] -> [batch_size, out_channels, 1, 1]
        time_proj = time_proj.unsqueeze(-1).unsqueeze(-1)
        h = h + time_proj
        
        # ç¬¬äºŒä¸ªå·ç§¯å—
        h = self.norm2(h)
        h = F.silu(h)
        h = self.conv2(h)
        
        # æ®‹å·®è¿æ¥
        return h + residual


class AttentionBlock(nn.Module):
    """
    è‡ªæ³¨æ„åŠ›å—
    
    åœ¨ç‰¹å¾å›¾çš„ç©ºé—´ç»´åº¦ä¸Šåº”ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œ
    å¸®åŠ©æ¨¡å‹æ•è·é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚
    """
    
    def __init__(self, channels: int, num_heads: int = 8):
        """
        åˆå§‹åŒ–æ³¨æ„åŠ›å—
        
        å‚æ•°:
            channels: é€šé“æ•°
            num_heads: æ³¨æ„åŠ›å¤´æ•°
        """
        super().__init__()
        self.channels = channels
        self.num_heads = num_heads
        self.head_dim = channels // num_heads
        
        assert channels % num_heads == 0, "channelså¿…é¡»èƒ½è¢«num_headsæ•´é™¤"
        
        # æ ‡å‡†åŒ–å±‚
        self.norm = nn.GroupNorm(8, channels)
        
        # æŸ¥è¯¢ã€é”®ã€å€¼æŠ•å½±å±‚
        self.to_qkv = nn.Conv2d(channels, channels * 3, 1)
        self.to_out = nn.Conv2d(channels, channels, 1)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        å‚æ•°:
            x: è¾“å…¥ç‰¹å¾å›¾ [batch_size, channels, height, width]
            
        è¿”å›:
            output: è¾“å‡ºç‰¹å¾å›¾ [batch_size, channels, height, width]
        """
        batch_size, channels, height, width = x.shape
        residual = x
        
        # æ ‡å‡†åŒ–
        x = self.norm(x)
        
        # è®¡ç®—QKV
        qkv = self.to_qkv(x)  # [B, C*3, H, W]
        
        # é‡å¡‘ä¸ºå¤šå¤´æ³¨æ„åŠ›æ ¼å¼
        qkv = qkv.view(batch_size, 3, self.num_heads, self.head_dim, height * width)
        qkv = qkv.permute(1, 0, 2, 4, 3)  # [3, B, num_heads, H*W, head_dim]
        q, k, v = qkv[0], qkv[1], qkv[2]
        
        # è®¡ç®—æ³¨æ„åŠ›æƒé‡
        # [B, num_heads, H*W, head_dim] @ [B, num_heads, head_dim, H*W] 
        # -> [B, num_heads, H*W, H*W]
        attention_scores = torch.matmul(q, k.transpose(-2, -1))
        attention_scores = attention_scores / math.sqrt(self.head_dim)
        attention_weights = F.softmax(attention_scores, dim=-1)
        
        # åº”ç”¨æ³¨æ„åŠ›æƒé‡
        # [B, num_heads, H*W, H*W] @ [B, num_heads, H*W, head_dim]
        # -> [B, num_heads, H*W, head_dim]
        out = torch.matmul(attention_weights, v)
        
        # é‡å¡‘å›åŸå§‹æ ¼å¼
        out = out.permute(0, 1, 3, 2)  # [B, num_heads, head_dim, H*W]
        out = out.contiguous().view(batch_size, channels, height, width)
        
        # è¾“å‡ºæŠ•å½±
        out = self.to_out(out)
        
        # æ®‹å·®è¿æ¥
        return out + residual


class DownBlock(nn.Module):
    """
    ä¸‹é‡‡æ ·å— - UNetç¼–ç å™¨çš„ç»„ä»¶
    
    åŒ…å«å¤šä¸ªæ®‹å·®å—å’Œå¯é€‰çš„æ³¨æ„åŠ›å—ï¼Œ
    æœ€åè¿›è¡Œä¸‹é‡‡æ ·æ“ä½œã€‚
    """
    
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        time_emb_dim: int,
        num_layers: int = 2,
        use_attention: bool = False,
        downsample: bool = True
    ):
        super().__init__()
        
        # æ®‹å·®å—åºåˆ—
        self.layers = nn.ModuleList()
        for i in range(num_layers):
            in_ch = in_channels if i == 0 else out_channels
            self.layers.append(
                ResidualBlock(in_ch, out_channels, time_emb_dim)
            )
            
            # åœ¨æœ€åä¸€å±‚æ·»åŠ æ³¨æ„åŠ›ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if use_attention and i == num_layers - 1:
                self.layers.append(AttentionBlock(out_channels))
        
        # ä¸‹é‡‡æ ·å±‚
        self.downsample = nn.Conv2d(out_channels, out_channels, 3, 2, 1) \
            if downsample else None
    
    def forward(
        self, 
        x: torch.Tensor, 
        time_emb: torch.Tensor
    ) -> Tuple[torch.Tensor, List[torch.Tensor]]:
        """
        å‰å‘ä¼ æ’­
        
        å‚æ•°:
            x: è¾“å…¥ç‰¹å¾å›¾
            time_emb: æ—¶é—´åµŒå…¥
            
        è¿”å›:
            x: è¾“å‡ºç‰¹å¾å›¾
            skip_connections: è·³è·ƒè¿æ¥ç‰¹å¾
        """
        skip_connections = []
        
        for layer in self.layers:
            if isinstance(layer, ResidualBlock):
                x = layer(x, time_emb)
            else:  # AttentionBlock
                x = layer(x)
            skip_connections.append(x)
        
        if self.downsample:
            x = self.downsample(x)
        
        return x, skip_connections


class UpBlock(nn.Module):
    """
    ä¸Šé‡‡æ ·å— - UNetè§£ç å™¨çš„ç»„ä»¶
    
    æ¥æ”¶è·³è·ƒè¿æ¥å¹¶è¿›è¡Œä¸Šé‡‡æ ·ï¼Œ
    æ¢å¤åŸå§‹åˆ†è¾¨ç‡ã€‚
    """
    
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        time_emb_dim: int,
        num_layers: int = 2,
        use_attention: bool = False,
        upsample: bool = True
    ):
        super().__init__()
        
        # ä¸Šé‡‡æ ·å±‚
        self.upsample = nn.ConvTranspose2d(in_channels, in_channels, 4, 2, 1) \
            if upsample else None
        
        # æ®‹å·®å—åºåˆ—
        self.layers = nn.ModuleList()
        for i in range(num_layers):
            # ç¬¬ä¸€å±‚éœ€è¦å¤„ç†è·³è·ƒè¿æ¥çš„æ‹¼æ¥
            in_ch = in_channels * 2 if i == 0 else out_channels
            self.layers.append(
                ResidualBlock(in_ch, out_channels, time_emb_dim)
            )
            
            # åœ¨æœ€åä¸€å±‚æ·»åŠ æ³¨æ„åŠ›ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if use_attention and i == num_layers - 1:
                self.layers.append(AttentionBlock(out_channels))
    
    def forward(
        self, 
        x: torch.Tensor, 
        skip_connections: List[torch.Tensor],
        time_emb: torch.Tensor
    ) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        å‚æ•°:
            x: è¾“å…¥ç‰¹å¾å›¾
            skip_connections: æ¥è‡ªç¼–ç å™¨çš„è·³è·ƒè¿æ¥
            time_emb: æ—¶é—´åµŒå…¥
            
        è¿”å›:
            x: è¾“å‡ºç‰¹å¾å›¾
        """
        if self.upsample:
            x = self.upsample(x)
        
        # é€†åºå¤„ç†è·³è·ƒè¿æ¥
        skip_connections = skip_connections[::-1]
        
        for i, layer in enumerate(self.layers):
            if isinstance(layer, ResidualBlock):
                if i == 0:
                    # ç¬¬ä¸€å±‚ï¼šæ‹¼æ¥è·³è·ƒè¿æ¥
                    skip = skip_connections[i] if i < len(skip_connections) else None
                    if skip is not None:
                        x = torch.cat([x, skip], dim=1)
                x = layer(x, time_emb)
            else:  # AttentionBlock
                x = layer(x)
        
        return x


class UNet(nn.Module):
    """
    å®Œæ•´çš„UNetæ¶æ„
    
    ç”¨äºæ‰©æ•£æ¨¡å‹çš„å™ªå£°é¢„æµ‹ç½‘ç»œã€‚
    é‡‡ç”¨ç¼–ç å™¨-è§£ç å™¨ç»“æ„ï¼Œå…·æœ‰è·³è·ƒè¿æ¥ã€‚
    """
    
    def __init__(
        self,
        in_channels: int = 3,
        out_channels: int = 3,
        channels: List[int] = [64, 128, 256, 512],
        num_layers: int = 2,
        attention_levels: List[bool] = [False, False, True, True],
        time_emb_dim: int = 512
    ):
        """
        åˆå§‹åŒ–UNet
        
        å‚æ•°:
            in_channels: è¾“å…¥é€šé“æ•°ï¼ˆRGBå›¾åƒä¸º3ï¼‰
            out_channels: è¾“å‡ºé€šé“æ•°ï¼ˆé€šå¸¸ç­‰äºè¾“å…¥é€šé“æ•°ï¼‰
            channels: å„å±‚çš„é€šé“æ•°é…ç½®
            num_layers: æ¯ä¸ªå—çš„æ®‹å·®å±‚æ•°
            attention_levels: å„å±‚æ˜¯å¦ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶
            time_emb_dim: æ—¶é—´åµŒå…¥ç»´åº¦
        """
        super().__init__()
        
        # æ—¶é—´åµŒå…¥å±‚
        self.time_embedding = TimeEmbedding(time_emb_dim)
        
        # è¾“å…¥æŠ•å½±
        self.input_conv = nn.Conv2d(in_channels, channels[0], 3, padding=1)
        
        # ç¼–ç å™¨ï¼ˆä¸‹é‡‡æ ·è·¯å¾„ï¼‰
        self.down_blocks = nn.ModuleList()
        for i in range(len(channels)):
            in_ch = channels[i-1] if i > 0 else channels[0]
            out_ch = channels[i]
            is_last = i == len(channels) - 1
            
            self.down_blocks.append(
                DownBlock(
                    in_ch, out_ch, time_emb_dim,
                    num_layers=num_layers,
                    use_attention=attention_levels[i],
                    downsample=not is_last
                )
            )
        
        # ä¸­é—´å—ï¼ˆæœ€æ·±å±‚ï¼‰
        mid_ch = channels[-1]
        self.mid_block = nn.Sequential(
            ResidualBlock(mid_ch, mid_ch, time_emb_dim),
            AttentionBlock(mid_ch),
            ResidualBlock(mid_ch, mid_ch, time_emb_dim)
        )
        
        # è§£ç å™¨ï¼ˆä¸Šé‡‡æ ·è·¯å¾„ï¼‰
        self.up_blocks = nn.ModuleList()
        reversed_channels = list(reversed(channels))
        reversed_attention = list(reversed(attention_levels))
        
        for i in range(len(reversed_channels)):
            in_ch = reversed_channels[i]
            out_ch = reversed_channels[i+1] if i < len(reversed_channels)-1 else channels[0]
            is_last = i == len(reversed_channels) - 1
            
            self.up_blocks.append(
                UpBlock(
                    in_ch, out_ch, time_emb_dim,
                    num_layers=num_layers + 1,  # +1å› ä¸ºè¦å¤„ç†è·³è·ƒè¿æ¥
                    use_attention=reversed_attention[i],
                    upsample=not is_last
                )
            )
        
        # è¾“å‡ºå±‚
        self.output_norm = nn.GroupNorm(8, channels[0])
        self.output_conv = nn.Conv2d(channels[0], out_channels, 3, padding=1)
    
    def forward(self, x: torch.Tensor, timesteps: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        å‚æ•°:
            x: è¾“å…¥å›¾åƒ [batch_size, in_channels, height, width]
            timesteps: æ—¶é—´æ­¥ [batch_size]
            
        è¿”å›:
            noise_pred: é¢„æµ‹çš„å™ªå£° [batch_size, out_channels, height, width]
        """
        # æ—¶é—´åµŒå…¥
        time_emb = self.time_embedding(timesteps)
        
        # è¾“å…¥æŠ•å½±
        x = self.input_conv(x)
        
        # ç¼–ç å™¨å‰å‘ä¼ æ’­ï¼Œæ”¶é›†è·³è·ƒè¿æ¥
        skip_connections_list = []
        for down_block in self.down_blocks:
            x, skip_connections = down_block(x, time_emb)
            skip_connections_list.extend(skip_connections)
        
        # ä¸­é—´å—
        for layer in self.mid_block:
            if isinstance(layer, ResidualBlock):
                x = layer(x, time_emb)
            else:  # AttentionBlock
                x = layer(x)
        
        # è§£ç å™¨å‰å‘ä¼ æ’­ï¼Œä½¿ç”¨è·³è·ƒè¿æ¥
        # å°†è·³è·ƒè¿æ¥æŒ‰è§£ç å™¨å—åˆ†ç»„
        skip_idx = len(skip_connections_list)
        for up_block in self.up_blocks:
            # ä¸ºæ¯ä¸ªä¸Šé‡‡æ ·å—åˆ†é…å¯¹åº”çš„è·³è·ƒè¿æ¥
            block_skip_connections = []
            num_layers = len([l for l in up_block.layers if isinstance(l, ResidualBlock)])
            
            # ä»åå¾€å‰å–è·³è·ƒè¿æ¥
            for _ in range(num_layers - 1):  # -1å› ä¸ºæœ€åä¸€å±‚ä¸éœ€è¦è·³è·ƒè¿æ¥
                if skip_idx > 0:
                    skip_idx -= 1
                    block_skip_connections.append(skip_connections_list[skip_idx])
            
            x = up_block(x, block_skip_connections, time_emb)
        
        # è¾“å‡ºå±‚
        x = self.output_norm(x)
        x = F.silu(x)
        x = self.output_conv(x)
        
        return x


# æµ‹è¯•å‡½æ•°
def test_unet():
    """
    æµ‹è¯•UNetç½‘ç»œçš„åŠŸèƒ½
    """
    print("ğŸ§ª æµ‹è¯•UNetç½‘ç»œ...")
    
    # åˆ›å»ºæ¨¡å‹
    model = UNet(
        in_channels=3,
        out_channels=3,
        channels=[64, 128, 256, 512],
        attention_levels=[False, False, True, True]
    )
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size = 2
    x = torch.randn(batch_size, 3, 64, 64)
    timesteps = torch.randint(0, 1000, (batch_size,))
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        output = model(x, timesteps)
    
    print(f"âœ… è¾“å…¥å½¢çŠ¶: {x.shape}")
    print(f"âœ… è¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"âœ… å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    print(f"âœ… å¯è®­ç»ƒå‚æ•°: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")


if __name__ == "__main__":
    test_unet()